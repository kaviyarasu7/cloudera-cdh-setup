


Solution:

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table categories --where "category_id between 1 and 22" --create-hive-table --hive-table default.categories --hive-import --m 1


=======================
Parqute and DF 

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table products --target-dir products_parquet --as-parquetfile

val products = sqlContext.read.parquet("products_parquet/");


=========================

Problem Scenario 35 : You have been given a file named spark7/EmployeeName.csv
(id,name).
EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat

1. Load this file from hdfs and sort it by name and save it back as (id,name) in results directory.
However, make sure while saving it should be able to write In a single file.

 val emp_data = sc.textFile("spark7/EmployeeName.csv")
 val emp_data_rdd = emp_data.map( row => (row.split(",")(1),(row.split(",")(0)))).sortByKey()
 emp_data_rdd.collect()
 val emp_data_rdd_id_name = emp_data_rdd.map(row => (row._2,row._1))
 emp_data_rdd_id_name.collect()
 emp_data_rdd_id_name.repartition(1).saveAsTextFile("spark7/results")

=============================

